{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7005d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably more imports than are really necessary...\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "import miditoolkit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48503b3f",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255b620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy1(groundtruth, predictions):\n",
    "    correct = 0\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        if predictions[k] == groundtruth[k]:\n",
    "            correct += 1\n",
    "    return correct / len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56e40fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy2(groundtruth, predictions):\n",
    "    correct = 0\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        if predictions[k] == groundtruth[k]:\n",
    "            correct += 1\n",
    "    return correct / len(groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f190f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['rock', 'oldies', 'jazz', 'pop', 'dance',  'blues',  'punk', 'chill', 'electronic', 'country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b772218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy3(groundtruth, predictions):\n",
    "    preds, targets = [], []\n",
    "    for k in groundtruth:\n",
    "        if not (k in predictions):\n",
    "            print(\"Missing \" + str(k) + \" from predictions\")\n",
    "            return 0\n",
    "        prediction = [1 if tag in predictions[k] else 0 for tag in TAGS]\n",
    "        target = [1 if tag in groundtruth[k] else 0 for tag in TAGS]\n",
    "        preds.append(prediction)\n",
    "        targets.append(target)\n",
    "    \n",
    "    mAP = average_precision_score(targets, preds, average='macro')\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487755c",
   "metadata": {},
   "source": [
    "## Task 1: Composer classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fdbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot1 = \"student_files/task1_composer_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b686224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model1():\n",
    "    def __init__(self):\n",
    "        # pass\n",
    "        self.scaler = None\n",
    "        self.le = None\n",
    "        self.model = None\n",
    "\n",
    "    def features(self, path):\n",
    "        filepath = path if os.path.exists(path) else os.path.join(dataroot1, path)\n",
    "        midi_obj = miditoolkit.midi.parser.MidiFile(filepath)\n",
    "        # midi_obj = miditoolkit.midi.parser.MidiFile(dataroot1 + '/' + path)\n",
    "        notes = midi_obj.instruments[0].notes\n",
    "        num_notes = len(notes)\n",
    "        \n",
    "        # avoid divide by zero\n",
    "        if num_notes == 0:\n",
    "            return [0.0, 0.0]\n",
    "\n",
    "        pitches = [note.pitch for note in notes]\n",
    "        durations = [note.end - note.start for note in notes]\n",
    "        total_time = max(note.end for note in notes) - min(note.start for note in notes)\n",
    "        \n",
    "        average_pitch = sum([note.pitch for note in notes]) / num_notes\n",
    "        std_pitch = np.std([note.pitch for note in notes])\n",
    "        average_duration = sum([note.end - note.start for note in notes]) / num_notes\n",
    "        std_duration = np.std(durations)\n",
    "        note_density = num_notes / total_time if total_time > 0 else 0\n",
    "        pitch_range = max(pitches) - min(pitches)\n",
    "\n",
    "        # Pitch class histogram\n",
    "        pitch_classes = [note.pitch % 12 for note in notes]\n",
    "        hist = [0] * 12\n",
    "        for pc in pitch_classes:\n",
    "            hist[pc] += 1\n",
    "        hist = [x / num_notes for x in hist] # normalize\n",
    "\n",
    "        # interval histogram\n",
    "        intervals = [pitches[i+1] - pitches[i] for i in range(len(pitches)-1)]\n",
    "        hist_intervals = [0] * 5\n",
    "        for interval in intervals:\n",
    "            if interval < -6:\n",
    "                hist_intervals[0] += 1\n",
    "            elif interval < 0:\n",
    "                hist_intervals[1] += 1\n",
    "            elif interval == 0:\n",
    "                hist_intervals[2] += 1\n",
    "            elif interval <= 6:\n",
    "                hist_intervals[3] += 1\n",
    "            else:\n",
    "                hist_intervals[4] += 1\n",
    "\n",
    "        if intervals:\n",
    "            hist_intervals = [x / len(intervals) for x in hist_intervals]\n",
    "        else:\n",
    "            hist_intervals = [0] * 5\n",
    "\n",
    "\n",
    "        # symbolic features\n",
    "        unique_pitches = len(set(pitches))\n",
    "        start_times = [note.start for note in notes]\n",
    "        unique_starts = len(set(start_times))\n",
    "        articulation_rate = unique_starts / total_time if total_time > 0 else 0\n",
    "\n",
    "        sorted_notes = sorted(notes, key=lambda n: n.start)\n",
    "        rest_time = 0.0\n",
    "        for i in range(1, len(sorted_notes)):\n",
    "            prev_end = sorted_notes[i-1].end\n",
    "            curr_start = sorted_notes[i].start\n",
    "            if curr_start > prev_end:\n",
    "                rest_time += curr_start - prev_end\n",
    "        rest_ratio = rest_time / total_time if total_time > 0 else 0\n",
    "\n",
    "        # polyphony\n",
    "        start_time_counts = {}\n",
    "        for note in notes:\n",
    "            t = round(note.start, 3)\n",
    "            start_time_counts[t] = start_time_counts.get(t,0) + 1\n",
    "        polyphonic_events = sum(1 for count in start_time_counts.values() if count > 1)\n",
    "        polyphony_ratio = polyphonic_events / len(start_time_counts) if start_time_counts else 0\n",
    "\n",
    "        # unique duration count\n",
    "        unique_durations = len(set(round(d, 3) for d in durations))\n",
    "\n",
    "        # longest rest\n",
    "        longest_rest = 0.0\n",
    "        for i in range(1, len(sorted_notes)):\n",
    "            gap = sorted_notes[i].start - sorted_notes[i-1].end\n",
    "            if gap > longest_rest:\n",
    "                longest_rest = gap\n",
    "\n",
    "\n",
    "        # note start times\n",
    "        onset_std = np.std(start_times) if len(start_times) > 1 else 0\n",
    "\n",
    "        # notes per beat\n",
    "        notes_per_beat = num_notes / midi_obj.ticks_per_beat if midi_obj.ticks_per_beat > 0 else 0\n",
    "\n",
    "        \n",
    "        # combined feature vector\n",
    "        features = [average_pitch, std_pitch, average_duration,\n",
    "                    std_duration, note_density, pitch_range] + hist + hist_intervals + [\n",
    "                        unique_pitches, articulation_rate, rest_ratio, \n",
    "                        polyphony_ratio, unique_durations, longest_rest,\n",
    "                        onset_std, notes_per_beat]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def predict(self, path, outpath=None):\n",
    "        d = eval(open(path, 'r').read())\n",
    "        predictions = {}\n",
    "        for k in d:\n",
    "            x = self.features(k)\n",
    "            x_scaled = self.scaler.transform([x])\n",
    "            raw = self.model.predict(x_scaled)\n",
    "            # pred = self.model.predict([x])\n",
    "            label = self.le.inverse_transform(raw)[0]\n",
    "            predictions[k] = label\n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                # z.write(str(predictions) + '\\n')\n",
    "                json.dump(predictions, z)\n",
    "        return predictions\n",
    "\n",
    "    # Train your model. Note that this function will not be called from the autograder:\n",
    "    # instead you should upload your saved model using save()\n",
    "    def train(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            train_json = eval(f.read())\n",
    "\n",
    "        # build feature matrix w labels\n",
    "        X = [self.features(k) for k in train_json]\n",
    "        y = [train_json[k] for k in train_json]\n",
    "\n",
    "        # label encode composer names\n",
    "        self.le = LabelEncoder()\n",
    "        y_enc = self.le.fit_transform(y)\n",
    "\n",
    "        # scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        # train/val split\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_scaled, y_enc, \n",
    "            test_size=0.2, \n",
    "            stratify=y_enc, \n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        \n",
    "        # fit XGBoost\n",
    "        self.model = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            n_estimators=500,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=1.0,\n",
    "            min_child_weight=1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        dtrain_full = xgb.DMatrix(X_scaled, label=y_enc)\n",
    "        params = self.model.get_xgb_params()\n",
    "        params['num_class'] = len(np.unique(y_enc))\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain_full,\n",
    "            num_boost_round=self.model.get_params()['n_estimators'],\n",
    "            nfold=5,\n",
    "            metrics='mlogloss',\n",
    "            early_stopping_rounds=20,\n",
    "            seed=42,\n",
    "            verbose_eval=True\n",
    "        )\n",
    "        best_n = len(cv_results)\n",
    "\n",
    "        # refit on full training data w optimal num of trees\n",
    "        self.model.set_params(n_estimators=best_n)\n",
    "        self.model.fit(X_scaled, y_enc)\n",
    "        \n",
    "        # self.model.fit(\n",
    "        #     X_tr, y_tr,\n",
    "        #     eval_set=[(X_val, y_val)],\n",
    "        #     verbose=True\n",
    "        # )\n",
    "        \n",
    "        # model.fit(X_train, y_train)\n",
    "        # self.model = model\n",
    "        # X_train = [self.features(k) for k in train_json]\n",
    "        # y_train = [train_json[k] for k in train_json]\n",
    "        # model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c83a314",
   "metadata": {},
   "source": [
    "## Task 2: Sequence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf9aaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot2 = \"student_files/task2_next_sequence_prediction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ac072a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model2():\n",
    "    def __init__(self):\n",
    "        self.scaler = None\n",
    "        self.model = XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            \n",
    "            n_estimators=1000,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.02,\n",
    "            \n",
    "            subsample=0.7,\n",
    "            colsample_bytree=0.7,\n",
    "\n",
    "            gamma=2.0,\n",
    "            min_child_weight=5,\n",
    "\n",
    "            reg_alpha=1.0,\n",
    "            reg_lambda=1.0,\n",
    "\n",
    "            tree_method='hist',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        self.base_rf = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # stack models\n",
    "        self.model = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb', self.base_xgb),\n",
    "                ('rf', self.base_rf),\n",
    "            ],\n",
    "            final_estimator=LogisticRegression(max_iter=1000),\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _js_divergence(p, q):\n",
    "        p, q = np.array(p), np.array(q)\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * (entropy(p, m) + entropy(q, m))\n",
    "\n",
    "    @staticmethod\n",
    "    def _pearson_corr(p, q):\n",
    "        p, q = np.array(p), np.array(q)\n",
    "        if p.std() == 0 or q.std() == 0:\n",
    "            return 0.0\n",
    "        return np.corrcoef(p, q)[0, 1]\n",
    "\n",
    "\n",
    "    def features(self, path):\n",
    "        # midi_obj = miditoolkit.midi.parser.MidiFile(dataroot2 + '/' + path)\n",
    "        filepath = path if os.path.exists(path) else os.path.join(dataroot2, path)\n",
    "        midi_obj = miditoolkit.midi.parser.MidiFile(filepath)\n",
    "        notes = midi_obj.instruments[0].notes\n",
    "        num_notes = len(notes)\n",
    "        \n",
    "        if num_notes == 0:\n",
    "            return [0]*31\n",
    "\n",
    "        pitches = [note.pitch for note in notes]\n",
    "        durations = [note.end - note.start for note in notes]\n",
    "        total_time = max(note.end for note in notes) - min(note.start for note in notes)\n",
    "        \n",
    "        average_pitch = sum([note.pitch for note in notes]) / num_notes\n",
    "        std_pitch = np.std([note.pitch for note in notes])\n",
    "        average_duration = sum([note.end - note.start for note in notes]) / num_notes\n",
    "        std_duration = np.std(durations)\n",
    "        note_density = num_notes / total_time if total_time > 0 else 0\n",
    "        pitch_range = max(pitches) - min(pitches)\n",
    "\n",
    "        # Pitch class histogram\n",
    "        pitch_classes = [note.pitch % 12 for note in notes]\n",
    "        hist = [0] * 12\n",
    "        for pc in pitch_classes:\n",
    "            hist[pc] += 1\n",
    "        hist = [x / num_notes for x in hist] # normalize\n",
    "\n",
    "        # interval histogram\n",
    "        intervals = [pitches[i+1] - pitches[i] for i in range(len(pitches)-1)]\n",
    "        hist_intervals = [0] * 5\n",
    "        for interval in intervals:\n",
    "            if interval < -6:\n",
    "                hist_intervals[0] += 1\n",
    "            elif interval < 0:\n",
    "                hist_intervals[1] += 1\n",
    "            elif interval == 0:\n",
    "                hist_intervals[2] += 1\n",
    "            elif interval <= 6:\n",
    "                hist_intervals[3] += 1\n",
    "            else:\n",
    "                hist_intervals[4] += 1\n",
    "\n",
    "        if intervals:\n",
    "            hist_intervals = [x / len(intervals) for x in hist_intervals]\n",
    "        else:\n",
    "            hist_intervals = [0] * 5\n",
    "\n",
    "\n",
    "        # symbolic features\n",
    "        unique_pitches = len(set(pitches))\n",
    "        start_times = [note.start for note in notes]\n",
    "        unique_starts = len(set(start_times))\n",
    "        articulation_rate = unique_starts / total_time if total_time > 0 else 0\n",
    "\n",
    "        sorted_notes = sorted(notes, key=lambda n: n.start)\n",
    "        rest_time = 0.0\n",
    "        for i in range(1, len(sorted_notes)):\n",
    "            prev_end = sorted_notes[i-1].end\n",
    "            curr_start = sorted_notes[i].start\n",
    "            if curr_start > prev_end:\n",
    "                rest_time += curr_start - prev_end\n",
    "        rest_ratio = rest_time / total_time if total_time > 0 else 0\n",
    "\n",
    "        # polyphony\n",
    "        start_time_counts = {}\n",
    "        for note in notes:\n",
    "            t = round(note.start, 3)\n",
    "            start_time_counts[t] = start_time_counts.get(t,0) + 1\n",
    "        polyphonic_events = sum(1 for count in start_time_counts.values() if count > 1)\n",
    "        polyphony_ratio = polyphonic_events / len(start_time_counts) if start_time_counts else 0\n",
    "\n",
    "        # unique duration count\n",
    "        unique_durations = len(set(round(d, 3) for d in durations))\n",
    "\n",
    "        # longest rest\n",
    "        longest_rest = 0.0\n",
    "        for i in range(1, len(sorted_notes)):\n",
    "            gap = sorted_notes[i].start - sorted_notes[i-1].end\n",
    "            if gap > longest_rest:\n",
    "                longest_rest = gap\n",
    "\n",
    "\n",
    "        # note start times\n",
    "        onset_std = np.std(start_times) if len(start_times) > 1 else 0\n",
    "\n",
    "        # notes per beat\n",
    "        notes_per_beat = num_notes / midi_obj.ticks_per_beat if midi_obj.ticks_per_beat > 0 else 0\n",
    "\n",
    "        \n",
    "        # combined feature vector\n",
    "        features = [average_pitch, std_pitch, average_duration,\n",
    "                    std_duration, note_density, pitch_range] + hist + hist_intervals + [\n",
    "                        unique_pitches, articulation_rate, rest_ratio, \n",
    "                        polyphony_ratio, unique_durations, longest_rest,\n",
    "                        onset_std, notes_per_beat]\n",
    "        \n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, path):\n",
    "        # This baseline doesn't use any model (it just measures feature similarity)\n",
    "        # You can use this approach but *probably* you'll want to implement a model\n",
    "        with open(path,'r') as f:\n",
    "            data = eval(f.read())\n",
    "\n",
    "        # build x and y\n",
    "        X_pairs, y = [], []\n",
    "        for (b1,b2), label in data.items():\n",
    "            f1 = self.features(b1)\n",
    "            f2 = self.features(b2)\n",
    "            # basic difference\n",
    "            diff = [abs(a-b) for a,b in zip(f1,f2)]\n",
    "            # elemnt product\n",
    "            prod = [a*b for a,b in zip(f1,f2)]\n",
    "            # squared dif\n",
    "            sqdiff = [(a-b)**2 for a,b in zip(f1,f2)]\n",
    "            # ratio\n",
    "            ratio = [a/(b+1e-6) for a,b in zip(f1,f2)]\n",
    "            \n",
    "            # distributional similarities\n",
    "            pc1, pc2 = f1[6:18], f2[6:18]\n",
    "            iv1, iv2 = f1[18:23], f2[18:23]\n",
    "            js_pc   = model2._js_divergence(pc1, pc2)\n",
    "            corr_pc = model2._pearson_corr(pc1, pc2)\n",
    "            js_iv   = model2._js_divergence(iv1, iv2)\n",
    "            corr_iv = model2._pearson_corr(iv1, iv2)\n",
    "\n",
    "            feat_vec = f1 + f2 + diff + prod + sqdiff + ratio + [js_pc, corr_pc, js_iv, corr_iv]\n",
    "            X_pairs.append(feat_vec)\n",
    "            y.append(int(label))\n",
    "\n",
    "        # 3 scale and split\n",
    "        self.scaler = StandardScaler()\n",
    "        Xs = self.scaler.fit_transform(X_pairs)\n",
    "        # X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        #     Xs, y, test_size=0.2, stratify=y, random_state=42\n",
    "        # )\n",
    "\n",
    "\n",
    "        # 4) use xgb.cv to find best n_estimators\n",
    "        dtrain_full = xgb.DMatrix(Xs, label=y)\n",
    "        params = self.model.get_xgb_params()\n",
    "        # esnure binary objective and metric\n",
    "        params['objective'] = 'binary:logistic'\n",
    "        params['eval_metric'] = 'logloss'\n",
    "\n",
    "        cv_results = xgb.cv(\n",
    "            params=params,\n",
    "            dtrain=dtrain_full,\n",
    "            num_boost_round=self.model.get_params()['n_estimators'],\n",
    "            nfold=5,\n",
    "            early_stopping_rounds=20,\n",
    "            seed=42,\n",
    "            verbose_eval=True\n",
    "        )\n",
    "        best_n = len(cv_results)\n",
    "\n",
    "        # 5. refit on all training data w optimal number of trees\n",
    "        self.model.set_params(n_estimators=best_n)\n",
    "        self.model.fit(Xs,y)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, path, outpath=None):\n",
    "        d = eval(open(path, 'r').read())\n",
    "        predictions = {}\n",
    "        for (b1,b2) in d:\n",
    "            # Keys are pairs of paths\n",
    "            f1 = self.features(b1)\n",
    "            f2 = self.features(b2)\n",
    "\n",
    "            diff = [abs(a-b) for a,b in zip(f1,f2)]\n",
    "            prod = [a*b for a,b in zip(f1,f2)]\n",
    "            sqdiff = [(a-b)**2 for a,b in zip(f1,f2)]\n",
    "            ratio = [a/(b+1e-6) for a,b in zip(f1,f2)]\n",
    "            # feature vector\n",
    "            feat_vec = f1 + f2 + diff + prod + sqdiff + ratio\n",
    "            pc1, pc2 = f1[6:18], f2[6:18]\n",
    "            iv1, iv2 = f1[18:23], f2[18:23]\n",
    "            js_pc   = model2._js_divergence(pc1, pc2)\n",
    "            corr_pc = model2._pearson_corr(pc1, pc2)\n",
    "            js_iv   = model2._js_divergence(iv1, iv2)\n",
    "            corr_iv = model2._pearson_corr(iv1, iv2)\n",
    "\n",
    "            feat_vec = f1 + f2 + diff + prod + sqdiff + ratio + [js_pc, corr_pc, js_iv, corr_iv]\n",
    "\n",
    "            \n",
    "            x = self.scaler.transform([feat_vec])\n",
    "            p = self.model.predict(x)[0]\n",
    "            predictions[(b1,b2)] = bool(p)\n",
    "            \n",
    "            # Note: hardcoded difference between features\n",
    "            # if abs(x1[0] - x2[0]) < 5:\n",
    "            #     predictions[k] = True\n",
    "            # else:\n",
    "            #     predictions[k] = False\n",
    "        if outpath:\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bf2cf",
   "metadata": {},
   "source": [
    "## Task 3: Audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c5c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants (you can change any of these if useful)\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 64\n",
    "N_CLASSES = 10\n",
    "AUDIO_DURATION = 10 # seconds\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot3 = \"student_files/task3_audio_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_waveform(path):\n",
    "    waveform, sr = librosa.load(dataroot3 + '/' + path, sr=SAMPLE_RATE)\n",
    "    waveform = np.array([waveform])\n",
    "    if sr != SAMPLE_RATE:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "        waveform = resample(waveform)\n",
    "    # Pad so that everything is the right length\n",
    "    target_len = SAMPLE_RATE * AUDIO_DURATION\n",
    "    if waveform.shape[1] < target_len:\n",
    "        pad_len = target_len - waveform.shape[1]\n",
    "        waveform = F.pad(waveform, (0, pad_len))\n",
    "    else:\n",
    "        waveform = waveform[:, :target_len]\n",
    "    waveform = torch.FloatTensor(waveform)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18660271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, meta, preload = True):\n",
    "        self.meta = meta\n",
    "        ks = list(meta.keys())\n",
    "        self.idToPath = dict(zip(range(len(ks)), ks))\n",
    "        self.pathToFeat = {}\n",
    "\n",
    "        self.mel = MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n",
    "        self.db = AmplitudeToDB()\n",
    "        \n",
    "        self.preload = preload # Determines whether the features should be preloaded (uses more memory)\n",
    "                               # or read from disk / computed each time (slow if your system is i/o-bound)\n",
    "        if self.preload:\n",
    "            for path in ks:\n",
    "                waveform = extract_waveform(path)\n",
    "                mel_spec = self.db(self.mel(waveform)).squeeze(0)\n",
    "                self.pathToFeat[path] = mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Faster version, preloads the features\n",
    "        path = self.idToPath[idx]\n",
    "        tags = self.meta[path]\n",
    "        bin_label = torch.tensor([1 if tag in tags else 0 for tag in TAGS], dtype=torch.float32)\n",
    "\n",
    "        if self.preload:\n",
    "            mel_spec = self.pathToFeat[path]\n",
    "        else:\n",
    "            waveform = extract_waveform(path)\n",
    "            mel_spec = self.db(self.mel(waveform)).squeeze(0)\n",
    "        \n",
    "        return mel_spec.unsqueeze(0), bin_label, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce87ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loaders():\n",
    "    def __init__(self, train_path, test_path, split_ratio=0.9, seed = 0):\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        meta_train = eval(open(train_path, 'r').read())\n",
    "        l_test = eval(open(test_path, 'r').read())\n",
    "        meta_test = dict([(x,[]) for x in l_test]) # Need a dictionary for the above class\n",
    "        \n",
    "        all_train = AudioDataset(meta_train)\n",
    "        test_set = AudioDataset(meta_test)\n",
    "        \n",
    "        # Split all_train into train + valid\n",
    "        total_len = len(all_train)\n",
    "        train_len = int(total_len * split_ratio)\n",
    "        valid_len = total_len - train_len\n",
    "        train_set, valid_set = random_split(all_train, [train_len, valid_len])\n",
    "        \n",
    "        self.loaderTrain = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderValid = DataLoader(valid_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        self.loaderTest = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d78705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, n_classes=N_CLASSES):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(32 * (N_MELS // 4) * (801 // 4), 256)\n",
    "        self.fc2 = nn.Linear(256, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (B, 16, mel/2, time/2)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 32, mel/4, time/4)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return torch.sigmoid(self.fc2(x))  # multilabel → sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e634810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    def __init__(self, model, learning_rate, seed = 0):\n",
    "        # These two lines will (mostly) make things deterministic.\n",
    "        # You're welcome to modify them to try to get a better solution.\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.device = torch.device(\"cpu\") # Can change this if you have a GPU, but the autograder will use CPU\n",
    "        self.model = model.to(self.device) #model.cuda() # Also uncomment these lines for GPU\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def evaluate(self, loader, threshold=0.5, outpath=None):\n",
    "        self.model.eval()\n",
    "        preds, targets, paths = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y, ps in loader:\n",
    "                x = x.to(self.device) #x.cuda()\n",
    "                y = y.to(self.device) #y.cuda()\n",
    "                outputs = self.model(x)\n",
    "                preds.append(outputs.cpu())\n",
    "                targets.append(y.cpu())\n",
    "                paths += list(ps)\n",
    "        \n",
    "        preds = torch.cat(preds)\n",
    "        targets = torch.cat(targets)\n",
    "        preds_bin = (preds > threshold).float()\n",
    "        \n",
    "        predictions = {}\n",
    "        for i in range(preds_bin.shape[0]):\n",
    "            predictions[paths[i]] = [TAGS[j] for j in range(len(preds_bin[i])) if preds_bin[i][j]]\n",
    "        \n",
    "        mAP = None\n",
    "        if outpath: # Save predictions\n",
    "            with open(outpath, \"w\") as z:\n",
    "                z.write(str(predictions) + '\\n')\n",
    "        else: # Only compute accuracy if we're *not* saving predictions, since we can't compute test accuracy\n",
    "            mAP = average_precision_score(targets, preds, average='macro')\n",
    "        return predictions, mAP\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for x, y, path in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                x = x.to(self.device) #x.cuda()\n",
    "                y = y.to(self.device) #y.cuda()\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(x)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            val_predictions, mAP = self.evaluate(val_loader)\n",
    "            print(f\"[Epoch {epoch+1}] Loss: {running_loss/len(train_loader):.4f} | Val mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87197d0",
   "metadata": {},
   "source": [
    "## Run everything..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9708d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run1():\n",
    "    model = model1()\n",
    "    model.train(dataroot1 + \"/train.json\")\n",
    "    train_preds = model.predict(dataroot1 + \"/train.json\")\n",
    "    test_preds = model.predict(dataroot1 + \"/test.json\", \"predictions1.json\")\n",
    "    \n",
    "    train_labels = eval(open(dataroot1 + \"/train.json\").read())\n",
    "    acc1 = accuracy1(train_labels, train_preds)\n",
    "    print(\"Task 1 training accuracy = \" + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9cb50b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run2():\n",
    "    model = model2()\n",
    "    model.train(dataroot2 + \"/train.json\")\n",
    "    train_preds = model.predict(dataroot2 + \"/train.json\")\n",
    "    test_preds = model.predict(dataroot2 + \"/test.json\", \"predictions2.json\")\n",
    "    \n",
    "    train_labels = eval(open(dataroot2 + \"/train.json\").read())\n",
    "    acc2 = accuracy2(train_labels, train_preds)\n",
    "    print(\"Task 2 training accuracy = \" + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run3():\n",
    "    loaders = Loaders(dataroot3 + \"/train.json\", dataroot3 + \"/test.json\")\n",
    "    model = CNNClassifier()\n",
    "    pipeline = Pipeline(model, 1e-4)\n",
    "    \n",
    "    pipeline.train(loaders.loaderTrain, loaders.loaderValid, 5)\n",
    "    train_preds, train_mAP = pipeline.evaluate(loaders.loaderTrain, 0.5)\n",
    "    valid_preds, valid_mAP = pipeline.evaluate(loaders.loaderValid, 0.5)\n",
    "    test_preds, _ = pipeline.evaluate(loaders.loaderTest, 0.5, \"predictions3.json\")\n",
    "    \n",
    "    all_train = eval(open(dataroot3 + \"/train.json\").read())\n",
    "    for k in valid_preds:\n",
    "        # We split our training set into train+valid\n",
    "        # so need to remove validation instances from the training set for evaluation\n",
    "        all_train.pop(k)\n",
    "    acc3 = accuracy3(all_train, train_preds)\n",
    "    print(\"Task 3 training mAP = \" + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "458d6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.97826+0.00187\ttest-mlogloss:2.01010+0.00234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulbulsara/anaconda3/envs/asmt1/lib/python3.10/site-packages/xgboost/training.py:209: UserWarning: [12:49:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1747336884418/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  return getattr(self.bst, name)(*args, **kwargs)\n",
      "/Users/rahulbulsara/anaconda3/envs/asmt1/lib/python3.10/site-packages/xgboost/training.py:215: UserWarning: [12:49:09] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1747336884418/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n",
      "/Users/rahulbulsara/anaconda3/envs/asmt1/lib/python3.10/site-packages/xgboost/training.py:215: UserWarning: [12:49:10] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1747336884418/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  self.bst.update(self.dtrain, iteration, fobj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-mlogloss:1.88550+0.00224\ttest-mlogloss:1.94568+0.00487\n",
      "[2]\ttrain-mlogloss:1.80005+0.00279\ttest-mlogloss:1.88817+0.00603\n",
      "[3]\ttrain-mlogloss:1.72172+0.00261\ttest-mlogloss:1.83740+0.00672\n",
      "[4]\ttrain-mlogloss:1.65056+0.00314\ttest-mlogloss:1.79370+0.00590\n",
      "[5]\ttrain-mlogloss:1.58340+0.00353\ttest-mlogloss:1.74875+0.00909\n",
      "[6]\ttrain-mlogloss:1.52210+0.00327\ttest-mlogloss:1.70979+0.01136\n",
      "[7]\ttrain-mlogloss:1.46275+0.00332\ttest-mlogloss:1.67013+0.01299\n",
      "[8]\ttrain-mlogloss:1.40752+0.00289\ttest-mlogloss:1.63400+0.01361\n",
      "[9]\ttrain-mlogloss:1.35640+0.00338\ttest-mlogloss:1.60117+0.01569\n",
      "[10]\ttrain-mlogloss:1.30781+0.00446\ttest-mlogloss:1.56835+0.01618\n",
      "[11]\ttrain-mlogloss:1.26144+0.00355\ttest-mlogloss:1.53811+0.01623\n",
      "[12]\ttrain-mlogloss:1.21829+0.00370\ttest-mlogloss:1.50968+0.01733\n",
      "[13]\ttrain-mlogloss:1.17727+0.00345\ttest-mlogloss:1.48272+0.01781\n",
      "[14]\ttrain-mlogloss:1.13732+0.00337\ttest-mlogloss:1.45853+0.01860\n",
      "[15]\ttrain-mlogloss:1.10004+0.00342\ttest-mlogloss:1.43376+0.01884\n",
      "[16]\ttrain-mlogloss:1.06557+0.00359\ttest-mlogloss:1.41271+0.01939\n",
      "[17]\ttrain-mlogloss:1.03179+0.00354\ttest-mlogloss:1.39163+0.02048\n",
      "[18]\ttrain-mlogloss:0.99976+0.00371\ttest-mlogloss:1.37144+0.01989\n",
      "[19]\ttrain-mlogloss:0.96937+0.00416\ttest-mlogloss:1.35273+0.01887\n",
      "[20]\ttrain-mlogloss:0.94041+0.00446\ttest-mlogloss:1.33296+0.01883\n",
      "[21]\ttrain-mlogloss:0.91288+0.00424\ttest-mlogloss:1.31625+0.01945\n",
      "[22]\ttrain-mlogloss:0.88649+0.00430\ttest-mlogloss:1.29950+0.02102\n",
      "[23]\ttrain-mlogloss:0.86080+0.00369\ttest-mlogloss:1.28374+0.02071\n",
      "[24]\ttrain-mlogloss:0.83641+0.00309\ttest-mlogloss:1.26813+0.02053\n",
      "[25]\ttrain-mlogloss:0.81295+0.00263\ttest-mlogloss:1.25328+0.02087\n",
      "[26]\ttrain-mlogloss:0.79072+0.00275\ttest-mlogloss:1.23805+0.02237\n",
      "[27]\ttrain-mlogloss:0.76893+0.00251\ttest-mlogloss:1.22389+0.02319\n",
      "[28]\ttrain-mlogloss:0.74856+0.00267\ttest-mlogloss:1.21190+0.02350\n",
      "[29]\ttrain-mlogloss:0.72872+0.00215\ttest-mlogloss:1.19855+0.02277\n",
      "[30]\ttrain-mlogloss:0.70973+0.00218\ttest-mlogloss:1.18654+0.02296\n",
      "[31]\ttrain-mlogloss:0.69178+0.00185\ttest-mlogloss:1.17519+0.02365\n",
      "[32]\ttrain-mlogloss:0.67490+0.00204\ttest-mlogloss:1.16437+0.02348\n",
      "[33]\ttrain-mlogloss:0.65812+0.00224\ttest-mlogloss:1.15321+0.02335\n",
      "[34]\ttrain-mlogloss:0.64237+0.00202\ttest-mlogloss:1.14295+0.02381\n",
      "[35]\ttrain-mlogloss:0.62711+0.00185\ttest-mlogloss:1.13387+0.02412\n",
      "[36]\ttrain-mlogloss:0.61187+0.00177\ttest-mlogloss:1.12451+0.02329\n",
      "[37]\ttrain-mlogloss:0.59753+0.00160\ttest-mlogloss:1.11459+0.02255\n",
      "[38]\ttrain-mlogloss:0.58410+0.00172\ttest-mlogloss:1.10602+0.02293\n",
      "[39]\ttrain-mlogloss:0.57144+0.00153\ttest-mlogloss:1.09695+0.02317\n",
      "[40]\ttrain-mlogloss:0.55842+0.00140\ttest-mlogloss:1.08856+0.02405\n",
      "[41]\ttrain-mlogloss:0.54655+0.00171\ttest-mlogloss:1.08071+0.02433\n",
      "[42]\ttrain-mlogloss:0.53535+0.00183\ttest-mlogloss:1.07215+0.02433\n",
      "[43]\ttrain-mlogloss:0.52401+0.00219\ttest-mlogloss:1.06482+0.02487\n",
      "[44]\ttrain-mlogloss:0.51302+0.00192\ttest-mlogloss:1.05749+0.02474\n",
      "[45]\ttrain-mlogloss:0.50247+0.00189\ttest-mlogloss:1.05086+0.02543\n",
      "[46]\ttrain-mlogloss:0.49251+0.00213\ttest-mlogloss:1.04339+0.02517\n",
      "[47]\ttrain-mlogloss:0.48265+0.00206\ttest-mlogloss:1.03656+0.02591\n",
      "[48]\ttrain-mlogloss:0.47335+0.00192\ttest-mlogloss:1.02991+0.02612\n",
      "[49]\ttrain-mlogloss:0.46401+0.00148\ttest-mlogloss:1.02303+0.02664\n",
      "[50]\ttrain-mlogloss:0.45537+0.00153\ttest-mlogloss:1.01718+0.02700\n",
      "[51]\ttrain-mlogloss:0.44673+0.00143\ttest-mlogloss:1.01157+0.02732\n",
      "[52]\ttrain-mlogloss:0.43880+0.00158\ttest-mlogloss:1.00631+0.02713\n",
      "[53]\ttrain-mlogloss:0.43082+0.00138\ttest-mlogloss:1.00093+0.02742\n",
      "[54]\ttrain-mlogloss:0.42344+0.00136\ttest-mlogloss:0.99540+0.02755\n",
      "[55]\ttrain-mlogloss:0.41654+0.00091\ttest-mlogloss:0.99018+0.02823\n",
      "[56]\ttrain-mlogloss:0.40969+0.00105\ttest-mlogloss:0.98527+0.02839\n",
      "[57]\ttrain-mlogloss:0.40338+0.00128\ttest-mlogloss:0.98034+0.02845\n",
      "[58]\ttrain-mlogloss:0.39687+0.00145\ttest-mlogloss:0.97558+0.02887\n",
      "[59]\ttrain-mlogloss:0.39117+0.00189\ttest-mlogloss:0.97161+0.02875\n",
      "[60]\ttrain-mlogloss:0.38521+0.00188\ttest-mlogloss:0.96724+0.02832\n",
      "[61]\ttrain-mlogloss:0.37939+0.00183\ttest-mlogloss:0.96298+0.02842\n",
      "[62]\ttrain-mlogloss:0.37369+0.00214\ttest-mlogloss:0.95911+0.02805\n",
      "[63]\ttrain-mlogloss:0.36825+0.00220\ttest-mlogloss:0.95483+0.02795\n",
      "[64]\ttrain-mlogloss:0.36299+0.00217\ttest-mlogloss:0.95059+0.02775\n",
      "[65]\ttrain-mlogloss:0.35792+0.00197\ttest-mlogloss:0.94681+0.02801\n",
      "[66]\ttrain-mlogloss:0.35299+0.00202\ttest-mlogloss:0.94318+0.02766\n",
      "[67]\ttrain-mlogloss:0.34826+0.00163\ttest-mlogloss:0.93986+0.02749\n",
      "[68]\ttrain-mlogloss:0.34389+0.00143\ttest-mlogloss:0.93660+0.02735\n",
      "[69]\ttrain-mlogloss:0.33939+0.00160\ttest-mlogloss:0.93336+0.02760\n",
      "[70]\ttrain-mlogloss:0.33540+0.00176\ttest-mlogloss:0.93030+0.02764\n",
      "[71]\ttrain-mlogloss:0.33144+0.00171\ttest-mlogloss:0.92687+0.02787\n",
      "[72]\ttrain-mlogloss:0.32780+0.00158\ttest-mlogloss:0.92421+0.02797\n",
      "[73]\ttrain-mlogloss:0.32417+0.00152\ttest-mlogloss:0.92145+0.02800\n",
      "[74]\ttrain-mlogloss:0.32026+0.00144\ttest-mlogloss:0.91780+0.02815\n",
      "[75]\ttrain-mlogloss:0.31700+0.00138\ttest-mlogloss:0.91489+0.02813\n",
      "[76]\ttrain-mlogloss:0.31369+0.00160\ttest-mlogloss:0.91190+0.02825\n",
      "[77]\ttrain-mlogloss:0.31041+0.00176\ttest-mlogloss:0.90916+0.02812\n",
      "[78]\ttrain-mlogloss:0.30748+0.00181\ttest-mlogloss:0.90683+0.02798\n",
      "[79]\ttrain-mlogloss:0.30462+0.00149\ttest-mlogloss:0.90420+0.02827\n",
      "[80]\ttrain-mlogloss:0.30197+0.00120\ttest-mlogloss:0.90219+0.02832\n",
      "[81]\ttrain-mlogloss:0.29923+0.00140\ttest-mlogloss:0.89954+0.02821\n",
      "[82]\ttrain-mlogloss:0.29654+0.00120\ttest-mlogloss:0.89759+0.02841\n",
      "[83]\ttrain-mlogloss:0.29385+0.00146\ttest-mlogloss:0.89508+0.02825\n",
      "[84]\ttrain-mlogloss:0.29125+0.00150\ttest-mlogloss:0.89285+0.02881\n",
      "[85]\ttrain-mlogloss:0.28915+0.00130\ttest-mlogloss:0.89119+0.02892\n",
      "[86]\ttrain-mlogloss:0.28694+0.00143\ttest-mlogloss:0.88940+0.02860\n",
      "[87]\ttrain-mlogloss:0.28470+0.00147\ttest-mlogloss:0.88760+0.02843\n",
      "[88]\ttrain-mlogloss:0.28282+0.00147\ttest-mlogloss:0.88586+0.02859\n",
      "[89]\ttrain-mlogloss:0.28065+0.00154\ttest-mlogloss:0.88368+0.02854\n",
      "[90]\ttrain-mlogloss:0.27899+0.00140\ttest-mlogloss:0.88224+0.02884\n",
      "[91]\ttrain-mlogloss:0.27705+0.00141\ttest-mlogloss:0.88088+0.02938\n",
      "[92]\ttrain-mlogloss:0.27498+0.00143\ttest-mlogloss:0.87922+0.02940\n",
      "[93]\ttrain-mlogloss:0.27343+0.00154\ttest-mlogloss:0.87781+0.02912\n",
      "[94]\ttrain-mlogloss:0.27183+0.00164\ttest-mlogloss:0.87634+0.02888\n",
      "[95]\ttrain-mlogloss:0.27037+0.00163\ttest-mlogloss:0.87477+0.02894\n",
      "[96]\ttrain-mlogloss:0.26858+0.00169\ttest-mlogloss:0.87338+0.02899\n",
      "[97]\ttrain-mlogloss:0.26696+0.00158\ttest-mlogloss:0.87198+0.02905\n",
      "[98]\ttrain-mlogloss:0.26547+0.00157\ttest-mlogloss:0.87087+0.02871\n",
      "[99]\ttrain-mlogloss:0.26426+0.00173\ttest-mlogloss:0.86975+0.02855\n",
      "[100]\ttrain-mlogloss:0.26298+0.00185\ttest-mlogloss:0.86882+0.02853\n",
      "[101]\ttrain-mlogloss:0.26147+0.00190\ttest-mlogloss:0.86758+0.02898\n",
      "[102]\ttrain-mlogloss:0.26013+0.00177\ttest-mlogloss:0.86654+0.02929\n",
      "[103]\ttrain-mlogloss:0.25888+0.00179\ttest-mlogloss:0.86540+0.02909\n",
      "[104]\ttrain-mlogloss:0.25795+0.00191\ttest-mlogloss:0.86455+0.02922\n",
      "[105]\ttrain-mlogloss:0.25666+0.00214\ttest-mlogloss:0.86349+0.02911\n",
      "[106]\ttrain-mlogloss:0.25550+0.00190\ttest-mlogloss:0.86249+0.02914\n",
      "[107]\ttrain-mlogloss:0.25443+0.00198\ttest-mlogloss:0.86191+0.02906\n",
      "[108]\ttrain-mlogloss:0.25355+0.00205\ttest-mlogloss:0.86111+0.02899\n",
      "[109]\ttrain-mlogloss:0.25226+0.00201\ttest-mlogloss:0.86004+0.02902\n",
      "[110]\ttrain-mlogloss:0.25125+0.00210\ttest-mlogloss:0.85899+0.02895\n",
      "[111]\ttrain-mlogloss:0.25027+0.00215\ttest-mlogloss:0.85805+0.02902\n",
      "[112]\ttrain-mlogloss:0.24929+0.00209\ttest-mlogloss:0.85713+0.02890\n",
      "[113]\ttrain-mlogloss:0.24852+0.00196\ttest-mlogloss:0.85634+0.02915\n",
      "[114]\ttrain-mlogloss:0.24783+0.00184\ttest-mlogloss:0.85590+0.02925\n",
      "[115]\ttrain-mlogloss:0.24706+0.00183\ttest-mlogloss:0.85532+0.02940\n",
      "[116]\ttrain-mlogloss:0.24627+0.00180\ttest-mlogloss:0.85471+0.02953\n",
      "[117]\ttrain-mlogloss:0.24570+0.00189\ttest-mlogloss:0.85434+0.02942\n",
      "[118]\ttrain-mlogloss:0.24483+0.00177\ttest-mlogloss:0.85346+0.02930\n",
      "[119]\ttrain-mlogloss:0.24398+0.00184\ttest-mlogloss:0.85261+0.02912\n",
      "[120]\ttrain-mlogloss:0.24333+0.00166\ttest-mlogloss:0.85187+0.02925\n",
      "[121]\ttrain-mlogloss:0.24266+0.00161\ttest-mlogloss:0.85117+0.02945\n",
      "[122]\ttrain-mlogloss:0.24202+0.00163\ttest-mlogloss:0.85079+0.02950\n",
      "[123]\ttrain-mlogloss:0.24147+0.00167\ttest-mlogloss:0.85026+0.02939\n",
      "[124]\ttrain-mlogloss:0.24085+0.00161\ttest-mlogloss:0.84986+0.02924\n",
      "[125]\ttrain-mlogloss:0.24034+0.00163\ttest-mlogloss:0.84935+0.02924\n",
      "[126]\ttrain-mlogloss:0.23945+0.00183\ttest-mlogloss:0.84860+0.02953\n",
      "[127]\ttrain-mlogloss:0.23891+0.00184\ttest-mlogloss:0.84831+0.02922\n",
      "[128]\ttrain-mlogloss:0.23848+0.00179\ttest-mlogloss:0.84784+0.02925\n",
      "[129]\ttrain-mlogloss:0.23800+0.00186\ttest-mlogloss:0.84732+0.02919\n",
      "[130]\ttrain-mlogloss:0.23746+0.00198\ttest-mlogloss:0.84675+0.02910\n",
      "[131]\ttrain-mlogloss:0.23686+0.00188\ttest-mlogloss:0.84597+0.02916\n",
      "[132]\ttrain-mlogloss:0.23632+0.00188\ttest-mlogloss:0.84572+0.02920\n",
      "[133]\ttrain-mlogloss:0.23566+0.00193\ttest-mlogloss:0.84525+0.02919\n",
      "[134]\ttrain-mlogloss:0.23509+0.00188\ttest-mlogloss:0.84465+0.02936\n",
      "[135]\ttrain-mlogloss:0.23465+0.00198\ttest-mlogloss:0.84437+0.02933\n",
      "[136]\ttrain-mlogloss:0.23433+0.00198\ttest-mlogloss:0.84426+0.02931\n",
      "[137]\ttrain-mlogloss:0.23393+0.00193\ttest-mlogloss:0.84388+0.02925\n",
      "[138]\ttrain-mlogloss:0.23354+0.00200\ttest-mlogloss:0.84347+0.02934\n",
      "[139]\ttrain-mlogloss:0.23312+0.00205\ttest-mlogloss:0.84320+0.02917\n",
      "[140]\ttrain-mlogloss:0.23282+0.00207\ttest-mlogloss:0.84288+0.02946\n",
      "[141]\ttrain-mlogloss:0.23239+0.00215\ttest-mlogloss:0.84260+0.02939\n",
      "[142]\ttrain-mlogloss:0.23193+0.00219\ttest-mlogloss:0.84202+0.02920\n",
      "[143]\ttrain-mlogloss:0.23164+0.00205\ttest-mlogloss:0.84182+0.02911\n",
      "[144]\ttrain-mlogloss:0.23128+0.00202\ttest-mlogloss:0.84162+0.02922\n",
      "[145]\ttrain-mlogloss:0.23091+0.00207\ttest-mlogloss:0.84120+0.02915\n",
      "[146]\ttrain-mlogloss:0.23052+0.00214\ttest-mlogloss:0.84095+0.02909\n",
      "[147]\ttrain-mlogloss:0.23023+0.00210\ttest-mlogloss:0.84073+0.02924\n",
      "[148]\ttrain-mlogloss:0.22984+0.00220\ttest-mlogloss:0.84048+0.02942\n",
      "[149]\ttrain-mlogloss:0.22958+0.00223\ttest-mlogloss:0.84015+0.02941\n",
      "[150]\ttrain-mlogloss:0.22932+0.00229\ttest-mlogloss:0.83992+0.02938\n",
      "[151]\ttrain-mlogloss:0.22911+0.00229\ttest-mlogloss:0.83976+0.02928\n",
      "[152]\ttrain-mlogloss:0.22890+0.00228\ttest-mlogloss:0.83964+0.02936\n",
      "[153]\ttrain-mlogloss:0.22872+0.00227\ttest-mlogloss:0.83953+0.02935\n",
      "[154]\ttrain-mlogloss:0.22853+0.00235\ttest-mlogloss:0.83936+0.02934\n",
      "[155]\ttrain-mlogloss:0.22823+0.00227\ttest-mlogloss:0.83910+0.02950\n",
      "[156]\ttrain-mlogloss:0.22806+0.00232\ttest-mlogloss:0.83902+0.02943\n",
      "[157]\ttrain-mlogloss:0.22765+0.00237\ttest-mlogloss:0.83868+0.02933\n",
      "[158]\ttrain-mlogloss:0.22739+0.00241\ttest-mlogloss:0.83841+0.02933\n",
      "[159]\ttrain-mlogloss:0.22710+0.00239\ttest-mlogloss:0.83827+0.02918\n",
      "[160]\ttrain-mlogloss:0.22694+0.00240\ttest-mlogloss:0.83828+0.02915\n",
      "[161]\ttrain-mlogloss:0.22659+0.00241\ttest-mlogloss:0.83801+0.02934\n",
      "[162]\ttrain-mlogloss:0.22634+0.00245\ttest-mlogloss:0.83789+0.02932\n",
      "[163]\ttrain-mlogloss:0.22615+0.00241\ttest-mlogloss:0.83793+0.02938\n",
      "[164]\ttrain-mlogloss:0.22583+0.00242\ttest-mlogloss:0.83751+0.02937\n",
      "[165]\ttrain-mlogloss:0.22557+0.00242\ttest-mlogloss:0.83734+0.02949\n",
      "[166]\ttrain-mlogloss:0.22538+0.00248\ttest-mlogloss:0.83721+0.02957\n",
      "[167]\ttrain-mlogloss:0.22525+0.00246\ttest-mlogloss:0.83703+0.02964\n",
      "[168]\ttrain-mlogloss:0.22506+0.00250\ttest-mlogloss:0.83687+0.02961\n",
      "[169]\ttrain-mlogloss:0.22477+0.00248\ttest-mlogloss:0.83670+0.02987\n",
      "[170]\ttrain-mlogloss:0.22458+0.00238\ttest-mlogloss:0.83647+0.02976\n",
      "[171]\ttrain-mlogloss:0.22438+0.00233\ttest-mlogloss:0.83627+0.02981\n",
      "[172]\ttrain-mlogloss:0.22417+0.00240\ttest-mlogloss:0.83613+0.02978\n",
      "[173]\ttrain-mlogloss:0.22404+0.00235\ttest-mlogloss:0.83599+0.02977\n",
      "[174]\ttrain-mlogloss:0.22394+0.00237\ttest-mlogloss:0.83596+0.02984\n",
      "[175]\ttrain-mlogloss:0.22382+0.00236\ttest-mlogloss:0.83590+0.02990\n",
      "[176]\ttrain-mlogloss:0.22354+0.00243\ttest-mlogloss:0.83575+0.03003\n",
      "[177]\ttrain-mlogloss:0.22339+0.00248\ttest-mlogloss:0.83545+0.02988\n",
      "[178]\ttrain-mlogloss:0.22330+0.00248\ttest-mlogloss:0.83534+0.02986\n",
      "[179]\ttrain-mlogloss:0.22322+0.00254\ttest-mlogloss:0.83516+0.02983\n",
      "[180]\ttrain-mlogloss:0.22308+0.00257\ttest-mlogloss:0.83521+0.02980\n",
      "[181]\ttrain-mlogloss:0.22299+0.00252\ttest-mlogloss:0.83518+0.02979\n",
      "[182]\ttrain-mlogloss:0.22288+0.00254\ttest-mlogloss:0.83495+0.02973\n",
      "[183]\ttrain-mlogloss:0.22272+0.00257\ttest-mlogloss:0.83475+0.02980\n",
      "[184]\ttrain-mlogloss:0.22257+0.00260\ttest-mlogloss:0.83477+0.02979\n",
      "[185]\ttrain-mlogloss:0.22252+0.00259\ttest-mlogloss:0.83470+0.02976\n",
      "[186]\ttrain-mlogloss:0.22239+0.00255\ttest-mlogloss:0.83465+0.02972\n",
      "[187]\ttrain-mlogloss:0.22226+0.00258\ttest-mlogloss:0.83449+0.02978\n",
      "[188]\ttrain-mlogloss:0.22208+0.00257\ttest-mlogloss:0.83414+0.02968\n",
      "[189]\ttrain-mlogloss:0.22189+0.00255\ttest-mlogloss:0.83399+0.02978\n",
      "[190]\ttrain-mlogloss:0.22171+0.00255\ttest-mlogloss:0.83380+0.02968\n",
      "[191]\ttrain-mlogloss:0.22158+0.00244\ttest-mlogloss:0.83361+0.02985\n",
      "[192]\ttrain-mlogloss:0.22142+0.00232\ttest-mlogloss:0.83360+0.02997\n",
      "[193]\ttrain-mlogloss:0.22137+0.00232\ttest-mlogloss:0.83354+0.03003\n",
      "[194]\ttrain-mlogloss:0.22130+0.00232\ttest-mlogloss:0.83350+0.02998\n",
      "[195]\ttrain-mlogloss:0.22116+0.00228\ttest-mlogloss:0.83335+0.02987\n",
      "[196]\ttrain-mlogloss:0.22100+0.00227\ttest-mlogloss:0.83340+0.02996\n",
      "[197]\ttrain-mlogloss:0.22074+0.00226\ttest-mlogloss:0.83321+0.02982\n",
      "[198]\ttrain-mlogloss:0.22056+0.00224\ttest-mlogloss:0.83299+0.02980\n",
      "[199]\ttrain-mlogloss:0.22046+0.00222\ttest-mlogloss:0.83302+0.02960\n",
      "[200]\ttrain-mlogloss:0.22033+0.00221\ttest-mlogloss:0.83305+0.02955\n",
      "[201]\ttrain-mlogloss:0.22023+0.00212\ttest-mlogloss:0.83285+0.02972\n",
      "[202]\ttrain-mlogloss:0.22007+0.00214\ttest-mlogloss:0.83264+0.02971\n",
      "[203]\ttrain-mlogloss:0.21999+0.00217\ttest-mlogloss:0.83253+0.02972\n",
      "[204]\ttrain-mlogloss:0.21990+0.00218\ttest-mlogloss:0.83249+0.02968\n",
      "[205]\ttrain-mlogloss:0.21979+0.00213\ttest-mlogloss:0.83235+0.02965\n",
      "[206]\ttrain-mlogloss:0.21968+0.00215\ttest-mlogloss:0.83227+0.02957\n",
      "[207]\ttrain-mlogloss:0.21952+0.00222\ttest-mlogloss:0.83210+0.02957\n",
      "[208]\ttrain-mlogloss:0.21932+0.00227\ttest-mlogloss:0.83227+0.02968\n",
      "[209]\ttrain-mlogloss:0.21922+0.00227\ttest-mlogloss:0.83219+0.02962\n",
      "[210]\ttrain-mlogloss:0.21903+0.00229\ttest-mlogloss:0.83222+0.02955\n",
      "[211]\ttrain-mlogloss:0.21889+0.00225\ttest-mlogloss:0.83204+0.02956\n",
      "[212]\ttrain-mlogloss:0.21880+0.00225\ttest-mlogloss:0.83200+0.02951\n",
      "[213]\ttrain-mlogloss:0.21873+0.00230\ttest-mlogloss:0.83199+0.02946\n",
      "[214]\ttrain-mlogloss:0.21861+0.00233\ttest-mlogloss:0.83188+0.02946\n",
      "[215]\ttrain-mlogloss:0.21858+0.00229\ttest-mlogloss:0.83182+0.02939\n",
      "[216]\ttrain-mlogloss:0.21852+0.00228\ttest-mlogloss:0.83171+0.02942\n",
      "[217]\ttrain-mlogloss:0.21842+0.00229\ttest-mlogloss:0.83180+0.02945\n",
      "[218]\ttrain-mlogloss:0.21826+0.00229\ttest-mlogloss:0.83168+0.02946\n",
      "[219]\ttrain-mlogloss:0.21813+0.00221\ttest-mlogloss:0.83161+0.02947\n",
      "[220]\ttrain-mlogloss:0.21805+0.00217\ttest-mlogloss:0.83168+0.02934\n",
      "[221]\ttrain-mlogloss:0.21797+0.00210\ttest-mlogloss:0.83151+0.02934\n",
      "[222]\ttrain-mlogloss:0.21793+0.00212\ttest-mlogloss:0.83147+0.02928\n",
      "[223]\ttrain-mlogloss:0.21783+0.00209\ttest-mlogloss:0.83140+0.02928\n",
      "[224]\ttrain-mlogloss:0.21773+0.00205\ttest-mlogloss:0.83134+0.02922\n",
      "[225]\ttrain-mlogloss:0.21757+0.00212\ttest-mlogloss:0.83137+0.02928\n",
      "[226]\ttrain-mlogloss:0.21752+0.00216\ttest-mlogloss:0.83129+0.02926\n",
      "[227]\ttrain-mlogloss:0.21747+0.00217\ttest-mlogloss:0.83125+0.02932\n",
      "[228]\ttrain-mlogloss:0.21735+0.00213\ttest-mlogloss:0.83100+0.02937\n",
      "[229]\ttrain-mlogloss:0.21722+0.00212\ttest-mlogloss:0.83082+0.02950\n",
      "[230]\ttrain-mlogloss:0.21714+0.00214\ttest-mlogloss:0.83076+0.02949\n",
      "[231]\ttrain-mlogloss:0.21702+0.00213\ttest-mlogloss:0.83076+0.02953\n",
      "[232]\ttrain-mlogloss:0.21684+0.00212\ttest-mlogloss:0.83056+0.02948\n",
      "[233]\ttrain-mlogloss:0.21679+0.00206\ttest-mlogloss:0.83058+0.02959\n",
      "[234]\ttrain-mlogloss:0.21674+0.00208\ttest-mlogloss:0.83047+0.02957\n",
      "[235]\ttrain-mlogloss:0.21667+0.00214\ttest-mlogloss:0.83042+0.02962\n",
      "[236]\ttrain-mlogloss:0.21662+0.00213\ttest-mlogloss:0.83045+0.02967\n",
      "[237]\ttrain-mlogloss:0.21656+0.00214\ttest-mlogloss:0.83040+0.02964\n",
      "[238]\ttrain-mlogloss:0.21651+0.00214\ttest-mlogloss:0.83030+0.02967\n",
      "[239]\ttrain-mlogloss:0.21643+0.00213\ttest-mlogloss:0.83027+0.02966\n",
      "[240]\ttrain-mlogloss:0.21635+0.00215\ttest-mlogloss:0.83028+0.02970\n",
      "[241]\ttrain-mlogloss:0.21626+0.00214\ttest-mlogloss:0.83025+0.02971\n",
      "[242]\ttrain-mlogloss:0.21615+0.00210\ttest-mlogloss:0.83008+0.02955\n",
      "[243]\ttrain-mlogloss:0.21605+0.00205\ttest-mlogloss:0.83007+0.02955\n",
      "[244]\ttrain-mlogloss:0.21601+0.00204\ttest-mlogloss:0.82998+0.02964\n",
      "[245]\ttrain-mlogloss:0.21594+0.00209\ttest-mlogloss:0.82990+0.02956\n",
      "[246]\ttrain-mlogloss:0.21588+0.00213\ttest-mlogloss:0.82979+0.02952\n",
      "[247]\ttrain-mlogloss:0.21581+0.00211\ttest-mlogloss:0.82977+0.02959\n",
      "[248]\ttrain-mlogloss:0.21575+0.00207\ttest-mlogloss:0.82969+0.02963\n",
      "[249]\ttrain-mlogloss:0.21559+0.00202\ttest-mlogloss:0.82947+0.02953\n",
      "[250]\ttrain-mlogloss:0.21548+0.00198\ttest-mlogloss:0.82939+0.02950\n",
      "[251]\ttrain-mlogloss:0.21544+0.00198\ttest-mlogloss:0.82940+0.02953\n",
      "[252]\ttrain-mlogloss:0.21540+0.00198\ttest-mlogloss:0.82936+0.02948\n",
      "[253]\ttrain-mlogloss:0.21534+0.00199\ttest-mlogloss:0.82931+0.02951\n",
      "[254]\ttrain-mlogloss:0.21525+0.00205\ttest-mlogloss:0.82932+0.02952\n",
      "[255]\ttrain-mlogloss:0.21518+0.00208\ttest-mlogloss:0.82925+0.02949\n",
      "[256]\ttrain-mlogloss:0.21516+0.00207\ttest-mlogloss:0.82927+0.02944\n",
      "[257]\ttrain-mlogloss:0.21513+0.00209\ttest-mlogloss:0.82916+0.02944\n",
      "[258]\ttrain-mlogloss:0.21505+0.00214\ttest-mlogloss:0.82920+0.02949\n",
      "[259]\ttrain-mlogloss:0.21497+0.00208\ttest-mlogloss:0.82928+0.02943\n",
      "[260]\ttrain-mlogloss:0.21488+0.00206\ttest-mlogloss:0.82915+0.02952\n",
      "[261]\ttrain-mlogloss:0.21481+0.00208\ttest-mlogloss:0.82913+0.02951\n",
      "[262]\ttrain-mlogloss:0.21471+0.00209\ttest-mlogloss:0.82903+0.02937\n",
      "[263]\ttrain-mlogloss:0.21464+0.00205\ttest-mlogloss:0.82904+0.02942\n",
      "[264]\ttrain-mlogloss:0.21459+0.00208\ttest-mlogloss:0.82904+0.02941\n",
      "[265]\ttrain-mlogloss:0.21447+0.00207\ttest-mlogloss:0.82901+0.02940\n",
      "[266]\ttrain-mlogloss:0.21446+0.00207\ttest-mlogloss:0.82898+0.02936\n",
      "[267]\ttrain-mlogloss:0.21442+0.00209\ttest-mlogloss:0.82897+0.02935\n",
      "[268]\ttrain-mlogloss:0.21434+0.00205\ttest-mlogloss:0.82891+0.02928\n",
      "[269]\ttrain-mlogloss:0.21429+0.00210\ttest-mlogloss:0.82885+0.02930\n",
      "[270]\ttrain-mlogloss:0.21428+0.00210\ttest-mlogloss:0.82884+0.02931\n",
      "[271]\ttrain-mlogloss:0.21421+0.00211\ttest-mlogloss:0.82868+0.02930\n",
      "[272]\ttrain-mlogloss:0.21411+0.00208\ttest-mlogloss:0.82850+0.02929\n",
      "[273]\ttrain-mlogloss:0.21407+0.00207\ttest-mlogloss:0.82853+0.02928\n",
      "[274]\ttrain-mlogloss:0.21405+0.00207\ttest-mlogloss:0.82855+0.02927\n",
      "[275]\ttrain-mlogloss:0.21398+0.00203\ttest-mlogloss:0.82847+0.02934\n",
      "[276]\ttrain-mlogloss:0.21386+0.00209\ttest-mlogloss:0.82841+0.02930\n",
      "[277]\ttrain-mlogloss:0.21376+0.00206\ttest-mlogloss:0.82843+0.02930\n",
      "[278]\ttrain-mlogloss:0.21370+0.00207\ttest-mlogloss:0.82854+0.02940\n",
      "[279]\ttrain-mlogloss:0.21370+0.00207\ttest-mlogloss:0.82852+0.02941\n",
      "[280]\ttrain-mlogloss:0.21367+0.00207\ttest-mlogloss:0.82848+0.02937\n",
      "[281]\ttrain-mlogloss:0.21360+0.00204\ttest-mlogloss:0.82851+0.02925\n",
      "[282]\ttrain-mlogloss:0.21354+0.00208\ttest-mlogloss:0.82849+0.02924\n",
      "[283]\ttrain-mlogloss:0.21347+0.00210\ttest-mlogloss:0.82845+0.02920\n",
      "[284]\ttrain-mlogloss:0.21334+0.00222\ttest-mlogloss:0.82831+0.02916\n",
      "[285]\ttrain-mlogloss:0.21330+0.00221\ttest-mlogloss:0.82827+0.02914\n",
      "[286]\ttrain-mlogloss:0.21325+0.00218\ttest-mlogloss:0.82824+0.02916\n",
      "[287]\ttrain-mlogloss:0.21321+0.00218\ttest-mlogloss:0.82813+0.02922\n",
      "[288]\ttrain-mlogloss:0.21312+0.00211\ttest-mlogloss:0.82791+0.02930\n",
      "[289]\ttrain-mlogloss:0.21310+0.00210\ttest-mlogloss:0.82792+0.02932\n",
      "[290]\ttrain-mlogloss:0.21304+0.00215\ttest-mlogloss:0.82781+0.02926\n",
      "[291]\ttrain-mlogloss:0.21301+0.00217\ttest-mlogloss:0.82788+0.02932\n",
      "[292]\ttrain-mlogloss:0.21297+0.00219\ttest-mlogloss:0.82782+0.02926\n",
      "[293]\ttrain-mlogloss:0.21293+0.00221\ttest-mlogloss:0.82779+0.02929\n",
      "[294]\ttrain-mlogloss:0.21288+0.00219\ttest-mlogloss:0.82772+0.02935\n",
      "[295]\ttrain-mlogloss:0.21285+0.00218\ttest-mlogloss:0.82769+0.02931\n",
      "[296]\ttrain-mlogloss:0.21280+0.00213\ttest-mlogloss:0.82755+0.02917\n",
      "[297]\ttrain-mlogloss:0.21272+0.00209\ttest-mlogloss:0.82752+0.02918\n",
      "[298]\ttrain-mlogloss:0.21270+0.00208\ttest-mlogloss:0.82748+0.02917\n",
      "[299]\ttrain-mlogloss:0.21266+0.00209\ttest-mlogloss:0.82741+0.02917\n",
      "[300]\ttrain-mlogloss:0.21261+0.00208\ttest-mlogloss:0.82738+0.02916\n",
      "[301]\ttrain-mlogloss:0.21260+0.00208\ttest-mlogloss:0.82739+0.02916\n",
      "[302]\ttrain-mlogloss:0.21249+0.00207\ttest-mlogloss:0.82724+0.02910\n",
      "[303]\ttrain-mlogloss:0.21246+0.00205\ttest-mlogloss:0.82720+0.02907\n",
      "[304]\ttrain-mlogloss:0.21242+0.00208\ttest-mlogloss:0.82711+0.02908\n",
      "[305]\ttrain-mlogloss:0.21238+0.00203\ttest-mlogloss:0.82711+0.02908\n",
      "[306]\ttrain-mlogloss:0.21231+0.00205\ttest-mlogloss:0.82712+0.02925\n",
      "[307]\ttrain-mlogloss:0.21228+0.00206\ttest-mlogloss:0.82710+0.02924\n",
      "[308]\ttrain-mlogloss:0.21221+0.00209\ttest-mlogloss:0.82705+0.02923\n",
      "[309]\ttrain-mlogloss:0.21213+0.00206\ttest-mlogloss:0.82701+0.02924\n",
      "[310]\ttrain-mlogloss:0.21213+0.00206\ttest-mlogloss:0.82702+0.02922\n",
      "[311]\ttrain-mlogloss:0.21210+0.00208\ttest-mlogloss:0.82702+0.02925\n",
      "[312]\ttrain-mlogloss:0.21204+0.00207\ttest-mlogloss:0.82702+0.02926\n",
      "[313]\ttrain-mlogloss:0.21199+0.00201\ttest-mlogloss:0.82696+0.02927\n",
      "[314]\ttrain-mlogloss:0.21195+0.00204\ttest-mlogloss:0.82695+0.02927\n",
      "[315]\ttrain-mlogloss:0.21192+0.00203\ttest-mlogloss:0.82688+0.02926\n",
      "[316]\ttrain-mlogloss:0.21188+0.00202\ttest-mlogloss:0.82690+0.02924\n",
      "[317]\ttrain-mlogloss:0.21178+0.00206\ttest-mlogloss:0.82676+0.02913\n",
      "[318]\ttrain-mlogloss:0.21173+0.00208\ttest-mlogloss:0.82675+0.02906\n",
      "[319]\ttrain-mlogloss:0.21170+0.00205\ttest-mlogloss:0.82672+0.02915\n",
      "[320]\ttrain-mlogloss:0.21169+0.00205\ttest-mlogloss:0.82669+0.02914\n",
      "[321]\ttrain-mlogloss:0.21166+0.00205\ttest-mlogloss:0.82673+0.02917\n",
      "[322]\ttrain-mlogloss:0.21163+0.00206\ttest-mlogloss:0.82675+0.02916\n",
      "[323]\ttrain-mlogloss:0.21162+0.00207\ttest-mlogloss:0.82675+0.02917\n",
      "[324]\ttrain-mlogloss:0.21154+0.00203\ttest-mlogloss:0.82658+0.02901\n",
      "[325]\ttrain-mlogloss:0.21151+0.00204\ttest-mlogloss:0.82648+0.02904\n",
      "[326]\ttrain-mlogloss:0.21148+0.00205\ttest-mlogloss:0.82640+0.02906\n",
      "[327]\ttrain-mlogloss:0.21146+0.00205\ttest-mlogloss:0.82642+0.02905\n",
      "[328]\ttrain-mlogloss:0.21137+0.00204\ttest-mlogloss:0.82636+0.02909\n",
      "[329]\ttrain-mlogloss:0.21133+0.00206\ttest-mlogloss:0.82627+0.02901\n",
      "[330]\ttrain-mlogloss:0.21127+0.00211\ttest-mlogloss:0.82625+0.02899\n",
      "[331]\ttrain-mlogloss:0.21123+0.00210\ttest-mlogloss:0.82618+0.02901\n",
      "[332]\ttrain-mlogloss:0.21114+0.00212\ttest-mlogloss:0.82606+0.02896\n",
      "[333]\ttrain-mlogloss:0.21107+0.00209\ttest-mlogloss:0.82591+0.02892\n",
      "[334]\ttrain-mlogloss:0.21102+0.00208\ttest-mlogloss:0.82583+0.02889\n",
      "[335]\ttrain-mlogloss:0.21100+0.00206\ttest-mlogloss:0.82584+0.02891\n",
      "[336]\ttrain-mlogloss:0.21098+0.00206\ttest-mlogloss:0.82582+0.02890\n",
      "[337]\ttrain-mlogloss:0.21093+0.00208\ttest-mlogloss:0.82585+0.02899\n",
      "[338]\ttrain-mlogloss:0.21093+0.00208\ttest-mlogloss:0.82587+0.02898\n",
      "[339]\ttrain-mlogloss:0.21092+0.00207\ttest-mlogloss:0.82591+0.02890\n",
      "[340]\ttrain-mlogloss:0.21088+0.00208\ttest-mlogloss:0.82573+0.02888\n",
      "[341]\ttrain-mlogloss:0.21078+0.00200\ttest-mlogloss:0.82563+0.02891\n",
      "[342]\ttrain-mlogloss:0.21073+0.00198\ttest-mlogloss:0.82563+0.02892\n",
      "[343]\ttrain-mlogloss:0.21073+0.00197\ttest-mlogloss:0.82561+0.02891\n",
      "[344]\ttrain-mlogloss:0.21069+0.00199\ttest-mlogloss:0.82559+0.02893\n",
      "[345]\ttrain-mlogloss:0.21066+0.00198\ttest-mlogloss:0.82552+0.02894\n",
      "[346]\ttrain-mlogloss:0.21054+0.00201\ttest-mlogloss:0.82550+0.02888\n",
      "[347]\ttrain-mlogloss:0.21054+0.00203\ttest-mlogloss:0.82551+0.02889\n",
      "[348]\ttrain-mlogloss:0.21040+0.00200\ttest-mlogloss:0.82543+0.02886\n",
      "[349]\ttrain-mlogloss:0.21033+0.00199\ttest-mlogloss:0.82541+0.02891\n",
      "[350]\ttrain-mlogloss:0.21021+0.00196\ttest-mlogloss:0.82540+0.02886\n",
      "[351]\ttrain-mlogloss:0.21017+0.00194\ttest-mlogloss:0.82534+0.02873\n",
      "[352]\ttrain-mlogloss:0.21017+0.00194\ttest-mlogloss:0.82534+0.02872\n",
      "[353]\ttrain-mlogloss:0.21015+0.00191\ttest-mlogloss:0.82527+0.02876\n",
      "[354]\ttrain-mlogloss:0.21011+0.00193\ttest-mlogloss:0.82520+0.02871\n",
      "[355]\ttrain-mlogloss:0.21008+0.00192\ttest-mlogloss:0.82527+0.02881\n",
      "[356]\ttrain-mlogloss:0.21007+0.00191\ttest-mlogloss:0.82522+0.02879\n",
      "[357]\ttrain-mlogloss:0.21004+0.00189\ttest-mlogloss:0.82519+0.02879\n",
      "[358]\ttrain-mlogloss:0.21001+0.00186\ttest-mlogloss:0.82519+0.02885\n",
      "[359]\ttrain-mlogloss:0.20997+0.00192\ttest-mlogloss:0.82522+0.02886\n",
      "[360]\ttrain-mlogloss:0.20995+0.00193\ttest-mlogloss:0.82519+0.02884\n",
      "[361]\ttrain-mlogloss:0.20991+0.00196\ttest-mlogloss:0.82513+0.02880\n",
      "[362]\ttrain-mlogloss:0.20986+0.00197\ttest-mlogloss:0.82507+0.02882\n",
      "[363]\ttrain-mlogloss:0.20985+0.00198\ttest-mlogloss:0.82508+0.02886\n",
      "[364]\ttrain-mlogloss:0.20984+0.00197\ttest-mlogloss:0.82507+0.02892\n",
      "[365]\ttrain-mlogloss:0.20980+0.00198\ttest-mlogloss:0.82502+0.02889\n",
      "[366]\ttrain-mlogloss:0.20978+0.00196\ttest-mlogloss:0.82496+0.02905\n",
      "[367]\ttrain-mlogloss:0.20975+0.00195\ttest-mlogloss:0.82497+0.02906\n",
      "[368]\ttrain-mlogloss:0.20968+0.00192\ttest-mlogloss:0.82494+0.02891\n",
      "[369]\ttrain-mlogloss:0.20963+0.00191\ttest-mlogloss:0.82503+0.02884\n",
      "[370]\ttrain-mlogloss:0.20961+0.00190\ttest-mlogloss:0.82494+0.02877\n",
      "[371]\ttrain-mlogloss:0.20960+0.00190\ttest-mlogloss:0.82491+0.02878\n",
      "[372]\ttrain-mlogloss:0.20960+0.00190\ttest-mlogloss:0.82492+0.02877\n",
      "[373]\ttrain-mlogloss:0.20959+0.00192\ttest-mlogloss:0.82491+0.02878\n",
      "[374]\ttrain-mlogloss:0.20956+0.00191\ttest-mlogloss:0.82496+0.02886\n",
      "[375]\ttrain-mlogloss:0.20951+0.00193\ttest-mlogloss:0.82490+0.02883\n",
      "[376]\ttrain-mlogloss:0.20942+0.00192\ttest-mlogloss:0.82489+0.02884\n",
      "[377]\ttrain-mlogloss:0.20939+0.00193\ttest-mlogloss:0.82484+0.02880\n",
      "[378]\ttrain-mlogloss:0.20938+0.00193\ttest-mlogloss:0.82485+0.02879\n",
      "[379]\ttrain-mlogloss:0.20936+0.00193\ttest-mlogloss:0.82481+0.02878\n",
      "[380]\ttrain-mlogloss:0.20932+0.00190\ttest-mlogloss:0.82482+0.02882\n",
      "[381]\ttrain-mlogloss:0.20930+0.00191\ttest-mlogloss:0.82477+0.02883\n",
      "[382]\ttrain-mlogloss:0.20926+0.00195\ttest-mlogloss:0.82466+0.02880\n",
      "[383]\ttrain-mlogloss:0.20923+0.00195\ttest-mlogloss:0.82457+0.02876\n",
      "[384]\ttrain-mlogloss:0.20918+0.00194\ttest-mlogloss:0.82447+0.02874\n",
      "[385]\ttrain-mlogloss:0.20918+0.00193\ttest-mlogloss:0.82447+0.02872\n",
      "[386]\ttrain-mlogloss:0.20915+0.00192\ttest-mlogloss:0.82440+0.02868\n",
      "[387]\ttrain-mlogloss:0.20915+0.00192\ttest-mlogloss:0.82445+0.02867\n",
      "[388]\ttrain-mlogloss:0.20904+0.00197\ttest-mlogloss:0.82429+0.02861\n",
      "[389]\ttrain-mlogloss:0.20902+0.00195\ttest-mlogloss:0.82431+0.02865\n",
      "[390]\ttrain-mlogloss:0.20899+0.00195\ttest-mlogloss:0.82430+0.02864\n",
      "[391]\ttrain-mlogloss:0.20897+0.00193\ttest-mlogloss:0.82435+0.02866\n",
      "[392]\ttrain-mlogloss:0.20894+0.00194\ttest-mlogloss:0.82430+0.02865\n",
      "[393]\ttrain-mlogloss:0.20888+0.00201\ttest-mlogloss:0.82429+0.02862\n",
      "[394]\ttrain-mlogloss:0.20885+0.00204\ttest-mlogloss:0.82429+0.02865\n",
      "[395]\ttrain-mlogloss:0.20884+0.00204\ttest-mlogloss:0.82429+0.02867\n",
      "[396]\ttrain-mlogloss:0.20878+0.00203\ttest-mlogloss:0.82422+0.02862\n",
      "[397]\ttrain-mlogloss:0.20874+0.00199\ttest-mlogloss:0.82420+0.02866\n",
      "[398]\ttrain-mlogloss:0.20871+0.00202\ttest-mlogloss:0.82420+0.02867\n",
      "[399]\ttrain-mlogloss:0.20868+0.00201\ttest-mlogloss:0.82414+0.02881\n",
      "[400]\ttrain-mlogloss:0.20864+0.00196\ttest-mlogloss:0.82400+0.02892\n",
      "[401]\ttrain-mlogloss:0.20863+0.00196\ttest-mlogloss:0.82407+0.02894\n",
      "[402]\ttrain-mlogloss:0.20852+0.00194\ttest-mlogloss:0.82399+0.02893\n",
      "[403]\ttrain-mlogloss:0.20847+0.00191\ttest-mlogloss:0.82395+0.02903\n",
      "[404]\ttrain-mlogloss:0.20844+0.00194\ttest-mlogloss:0.82400+0.02906\n",
      "[405]\ttrain-mlogloss:0.20842+0.00194\ttest-mlogloss:0.82396+0.02905\n",
      "[406]\ttrain-mlogloss:0.20839+0.00197\ttest-mlogloss:0.82398+0.02904\n",
      "[407]\ttrain-mlogloss:0.20837+0.00196\ttest-mlogloss:0.82399+0.02899\n",
      "[408]\ttrain-mlogloss:0.20836+0.00198\ttest-mlogloss:0.82400+0.02899\n",
      "[409]\ttrain-mlogloss:0.20833+0.00201\ttest-mlogloss:0.82396+0.02896\n",
      "[410]\ttrain-mlogloss:0.20832+0.00203\ttest-mlogloss:0.82394+0.02895\n",
      "[411]\ttrain-mlogloss:0.20828+0.00205\ttest-mlogloss:0.82396+0.02902\n",
      "[412]\ttrain-mlogloss:0.20820+0.00203\ttest-mlogloss:0.82387+0.02898\n",
      "[413]\ttrain-mlogloss:0.20816+0.00203\ttest-mlogloss:0.82387+0.02903\n",
      "[414]\ttrain-mlogloss:0.20812+0.00206\ttest-mlogloss:0.82381+0.02900\n",
      "[415]\ttrain-mlogloss:0.20807+0.00202\ttest-mlogloss:0.82383+0.02892\n",
      "[416]\ttrain-mlogloss:0.20800+0.00204\ttest-mlogloss:0.82392+0.02897\n",
      "[417]\ttrain-mlogloss:0.20797+0.00203\ttest-mlogloss:0.82394+0.02902\n",
      "[418]\ttrain-mlogloss:0.20795+0.00204\ttest-mlogloss:0.82392+0.02902\n",
      "[419]\ttrain-mlogloss:0.20792+0.00206\ttest-mlogloss:0.82383+0.02896\n",
      "[420]\ttrain-mlogloss:0.20790+0.00204\ttest-mlogloss:0.82382+0.02904\n",
      "[421]\ttrain-mlogloss:0.20789+0.00205\ttest-mlogloss:0.82378+0.02903\n",
      "[422]\ttrain-mlogloss:0.20787+0.00205\ttest-mlogloss:0.82378+0.02903\n",
      "[423]\ttrain-mlogloss:0.20784+0.00201\ttest-mlogloss:0.82371+0.02903\n",
      "[424]\ttrain-mlogloss:0.20784+0.00201\ttest-mlogloss:0.82369+0.02897\n",
      "[425]\ttrain-mlogloss:0.20783+0.00203\ttest-mlogloss:0.82369+0.02896\n",
      "[426]\ttrain-mlogloss:0.20781+0.00201\ttest-mlogloss:0.82370+0.02891\n",
      "[427]\ttrain-mlogloss:0.20780+0.00201\ttest-mlogloss:0.82378+0.02892\n",
      "[428]\ttrain-mlogloss:0.20772+0.00204\ttest-mlogloss:0.82385+0.02896\n",
      "[429]\ttrain-mlogloss:0.20771+0.00203\ttest-mlogloss:0.82386+0.02892\n",
      "[430]\ttrain-mlogloss:0.20766+0.00201\ttest-mlogloss:0.82378+0.02904\n",
      "[431]\ttrain-mlogloss:0.20762+0.00203\ttest-mlogloss:0.82373+0.02910\n",
      "[432]\ttrain-mlogloss:0.20755+0.00206\ttest-mlogloss:0.82366+0.02910\n",
      "[433]\ttrain-mlogloss:0.20752+0.00203\ttest-mlogloss:0.82365+0.02913\n",
      "[434]\ttrain-mlogloss:0.20749+0.00204\ttest-mlogloss:0.82363+0.02913\n",
      "[435]\ttrain-mlogloss:0.20747+0.00205\ttest-mlogloss:0.82353+0.02917\n",
      "[436]\ttrain-mlogloss:0.20745+0.00207\ttest-mlogloss:0.82350+0.02919\n",
      "[437]\ttrain-mlogloss:0.20741+0.00207\ttest-mlogloss:0.82343+0.02918\n",
      "[438]\ttrain-mlogloss:0.20733+0.00207\ttest-mlogloss:0.82330+0.02907\n",
      "[439]\ttrain-mlogloss:0.20731+0.00207\ttest-mlogloss:0.82328+0.02908\n",
      "[440]\ttrain-mlogloss:0.20728+0.00203\ttest-mlogloss:0.82325+0.02910\n",
      "[441]\ttrain-mlogloss:0.20721+0.00196\ttest-mlogloss:0.82322+0.02912\n",
      "[442]\ttrain-mlogloss:0.20720+0.00198\ttest-mlogloss:0.82315+0.02908\n",
      "[443]\ttrain-mlogloss:0.20717+0.00198\ttest-mlogloss:0.82304+0.02912\n",
      "[444]\ttrain-mlogloss:0.20715+0.00197\ttest-mlogloss:0.82303+0.02916\n",
      "[445]\ttrain-mlogloss:0.20711+0.00197\ttest-mlogloss:0.82300+0.02916\n",
      "[446]\ttrain-mlogloss:0.20708+0.00199\ttest-mlogloss:0.82294+0.02914\n",
      "[447]\ttrain-mlogloss:0.20705+0.00199\ttest-mlogloss:0.82289+0.02907\n",
      "[448]\ttrain-mlogloss:0.20701+0.00199\ttest-mlogloss:0.82275+0.02901\n",
      "[449]\ttrain-mlogloss:0.20697+0.00196\ttest-mlogloss:0.82278+0.02899\n",
      "[450]\ttrain-mlogloss:0.20695+0.00196\ttest-mlogloss:0.82272+0.02898\n",
      "[451]\ttrain-mlogloss:0.20695+0.00198\ttest-mlogloss:0.82274+0.02903\n",
      "[452]\ttrain-mlogloss:0.20694+0.00198\ttest-mlogloss:0.82272+0.02900\n",
      "[453]\ttrain-mlogloss:0.20692+0.00197\ttest-mlogloss:0.82270+0.02899\n",
      "[454]\ttrain-mlogloss:0.20689+0.00196\ttest-mlogloss:0.82271+0.02899\n",
      "[455]\ttrain-mlogloss:0.20685+0.00193\ttest-mlogloss:0.82258+0.02895\n",
      "[456]\ttrain-mlogloss:0.20678+0.00197\ttest-mlogloss:0.82254+0.02898\n",
      "[457]\ttrain-mlogloss:0.20674+0.00197\ttest-mlogloss:0.82255+0.02891\n",
      "[458]\ttrain-mlogloss:0.20669+0.00192\ttest-mlogloss:0.82247+0.02885\n",
      "[459]\ttrain-mlogloss:0.20667+0.00191\ttest-mlogloss:0.82245+0.02881\n",
      "[460]\ttrain-mlogloss:0.20662+0.00193\ttest-mlogloss:0.82233+0.02880\n",
      "[461]\ttrain-mlogloss:0.20660+0.00192\ttest-mlogloss:0.82241+0.02882\n",
      "[462]\ttrain-mlogloss:0.20660+0.00192\ttest-mlogloss:0.82241+0.02881\n",
      "[463]\ttrain-mlogloss:0.20655+0.00193\ttest-mlogloss:0.82233+0.02878\n",
      "[464]\ttrain-mlogloss:0.20651+0.00191\ttest-mlogloss:0.82217+0.02875\n",
      "[465]\ttrain-mlogloss:0.20647+0.00190\ttest-mlogloss:0.82222+0.02880\n",
      "[466]\ttrain-mlogloss:0.20647+0.00190\ttest-mlogloss:0.82223+0.02879\n",
      "[467]\ttrain-mlogloss:0.20643+0.00192\ttest-mlogloss:0.82223+0.02883\n",
      "[468]\ttrain-mlogloss:0.20640+0.00189\ttest-mlogloss:0.82212+0.02870\n",
      "[469]\ttrain-mlogloss:0.20637+0.00189\ttest-mlogloss:0.82217+0.02875\n",
      "[470]\ttrain-mlogloss:0.20633+0.00195\ttest-mlogloss:0.82217+0.02875\n",
      "[471]\ttrain-mlogloss:0.20627+0.00200\ttest-mlogloss:0.82200+0.02870\n",
      "[472]\ttrain-mlogloss:0.20626+0.00199\ttest-mlogloss:0.82200+0.02871\n",
      "[473]\ttrain-mlogloss:0.20625+0.00198\ttest-mlogloss:0.82200+0.02870\n",
      "[474]\ttrain-mlogloss:0.20624+0.00198\ttest-mlogloss:0.82200+0.02869\n",
      "[475]\ttrain-mlogloss:0.20620+0.00197\ttest-mlogloss:0.82200+0.02865\n",
      "[476]\ttrain-mlogloss:0.20617+0.00196\ttest-mlogloss:0.82200+0.02861\n",
      "[477]\ttrain-mlogloss:0.20615+0.00196\ttest-mlogloss:0.82189+0.02855\n",
      "[478]\ttrain-mlogloss:0.20613+0.00198\ttest-mlogloss:0.82191+0.02857\n",
      "[479]\ttrain-mlogloss:0.20609+0.00194\ttest-mlogloss:0.82185+0.02866\n",
      "[480]\ttrain-mlogloss:0.20607+0.00192\ttest-mlogloss:0.82189+0.02866\n",
      "[481]\ttrain-mlogloss:0.20607+0.00192\ttest-mlogloss:0.82190+0.02867\n",
      "[482]\ttrain-mlogloss:0.20604+0.00191\ttest-mlogloss:0.82195+0.02874\n",
      "[483]\ttrain-mlogloss:0.20599+0.00189\ttest-mlogloss:0.82196+0.02874\n",
      "[484]\ttrain-mlogloss:0.20597+0.00190\ttest-mlogloss:0.82190+0.02881\n",
      "[485]\ttrain-mlogloss:0.20594+0.00194\ttest-mlogloss:0.82188+0.02881\n",
      "[486]\ttrain-mlogloss:0.20591+0.00192\ttest-mlogloss:0.82179+0.02879\n",
      "[487]\ttrain-mlogloss:0.20588+0.00190\ttest-mlogloss:0.82175+0.02877\n",
      "[488]\ttrain-mlogloss:0.20585+0.00189\ttest-mlogloss:0.82166+0.02878\n",
      "[489]\ttrain-mlogloss:0.20584+0.00190\ttest-mlogloss:0.82168+0.02879\n",
      "[490]\ttrain-mlogloss:0.20580+0.00190\ttest-mlogloss:0.82162+0.02874\n",
      "[491]\ttrain-mlogloss:0.20578+0.00191\ttest-mlogloss:0.82166+0.02870\n",
      "[492]\ttrain-mlogloss:0.20578+0.00191\ttest-mlogloss:0.82164+0.02868\n",
      "[493]\ttrain-mlogloss:0.20574+0.00192\ttest-mlogloss:0.82159+0.02861\n",
      "[494]\ttrain-mlogloss:0.20573+0.00191\ttest-mlogloss:0.82157+0.02862\n",
      "[495]\ttrain-mlogloss:0.20570+0.00193\ttest-mlogloss:0.82157+0.02860\n",
      "[496]\ttrain-mlogloss:0.20565+0.00197\ttest-mlogloss:0.82149+0.02860\n",
      "[497]\ttrain-mlogloss:0.20565+0.00197\ttest-mlogloss:0.82151+0.02861\n",
      "[498]\ttrain-mlogloss:0.20564+0.00198\ttest-mlogloss:0.82153+0.02861\n",
      "[499]\ttrain-mlogloss:0.20562+0.00198\ttest-mlogloss:0.82152+0.02869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulbulsara/anaconda3/envs/asmt1/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [12:49:25] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1747336884418/work/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1 training accuracy = 0.8644628099173554\n"
     ]
    }
   ],
   "source": [
    "run1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cdd32b83-9240-44b3-aa48-0e9951432202",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'model2' object has no attribute 'base_xgb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[114], line 2\u001b[0m, in \u001b[0;36mrun2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun2\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(dataroot2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     train_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(dataroot2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[128], line 35\u001b[0m, in \u001b[0;36mmodel2.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     27\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     28\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     29\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# stack models\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m StackingClassifier(\n\u001b[1;32m     34\u001b[0m         estimators\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 35\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_xgb\u001b[49m),\n\u001b[1;32m     36\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_rf),\n\u001b[1;32m     37\u001b[0m         ],\n\u001b[1;32m     38\u001b[0m         final_estimator\u001b[38;5;241m=\u001b[39mLogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m),\n\u001b[1;32m     39\u001b[0m         cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     40\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'model2' object has no attribute 'base_xgb'"
     ]
    }
   ],
   "source": [
    "run2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8b523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "153 (asmt1)",
   "language": "python",
   "name": "asmt1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
